# -*- coding: utf-8 -*-
"""AnswerMate_PDF_BOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xIdcGUR5uTZ9z0NPls8jqHXc3UMY0pl8
"""

!pip install fastapi uvicorn pyMuPDF faiss-cpu sentence-transformers openai nest-asyncio requests

!pip install python-multipart

# Import required libraries
from fastapi import FastAPI, UploadFile, File, HTTPException
import fitz  # PyMuPDF for PDF text extraction
import openai
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import os
import json
from typing import List
import nest_asyncio
import uvicorn
import threading

# Initialize FastAPI app
app = FastAPI()

# Apply nest_asyncio to allow running FastAPI in Colab
nest_asyncio.apply()

# Load OpenAI API key from environment variable
openai.api_key = os.getenv("OPENAI_API_KEY")

# Load Sentence Transformer model for embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# FAISS index for vector search
dimension = 384  # Embedding size for the model
index = faiss.IndexFlatL2(dimension)
documents = []  # Store document text

# Function to extract text from PDF
def extract_text_from_pdf(pdf_file):
    """Extract text from an uploaded PDF file."""
    doc = fitz.open(stream=pdf_file.file.read(), filetype="pdf")
    text = "\n".join([page.get_text("text") for page in doc])
    return text

# Endpoint to upload and process PDFs
@app.post("/upload/")
def upload_pdf(file: UploadFile = File(...)):
    """Uploads a PDF, extracts text, generates embeddings, and stores it."""
    if not file.filename.endswith(".pdf"):
        raise HTTPException(status_code=400, detail="Only PDF files are allowed")

    text = extract_text_from_pdf(file)
    if not text.strip():
        raise HTTPException(status_code=400, detail="No text found in PDF")

    # Generate embeddings
    text_embeddings = embedding_model.encode([text])

    # Store document and add embedding to FAISS index
    doc_id = len(documents)
    documents.append(text)
    index.add(np.array(text_embeddings, dtype=np.float32))

    return {"message": "PDF uploaded and processed successfully", "doc_id": doc_id}

# Endpoint to query chatbot
@app.post("/chat/")
def chat_with_bot(query: str):
    """Handles user queries, retrieves relevant document text, and generates responses using GPT."""
    if not documents:
        raise HTTPException(status_code=400, detail="No documents uploaded")

    # Convert query to embedding and find the most relevant document
    query_embedding = embedding_model.encode([query])
    D, I = index.search(np.array(query_embedding, dtype=np.float32), k=1)

    # Retrieve the most relevant document
    relevant_doc = documents[I[0][0]] if I[0][0] >= 0 else "No relevant document found"

    # Use OpenAI GPT to generate a response
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an AI assistant answering questions based on a document."},
            {"role": "user", "content": f"Document: {relevant_doc}\n\nQuestion: {query}"}
        ]
    )

    return {"answer": response['choices'][0]['message']['content']}

# Function to run FastAPI inside Google Colab
def run():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Run FastAPI in a background thread
threading.Thread(target=run, daemon=True).start()

import requests

response = requests.get("http://0.0.0.0:8000/docs")
print(response.status_code)

import requests

files = {"file": open("/content/drive/MyDrive/Nlp tasks/1. The Metamorphosis, Franz Kafka.pdf", "rb")}  # Replace with your actual PDF
response = requests.post("http://0.0.0.0:8000/upload/", files=files)
print(response.json())

response = requests.post("http://0.0.0.0:8000/chat/", json={"query": "How many pendrive?"})
print(response.json())

from fastapi import FastAPI, UploadFile, File, HTTPException
import fitz  # PyMuPDF for PDF text extraction
import openai
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import os
import json
from typing import List

# Initialize FastAPI app
app = FastAPI()

# Load OpenAI API key (ensure it's set in environment variables)
openai.api_key = os.getenv("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")

# Load Sentence Transformer model for embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# FAISS index for vector search
dimension = 384  # Embedding size
index = faiss.IndexFlatL2(dimension)
documents = []  # Store text data

# Function to extract text from PDF
def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.file.read(), filetype="pdf")
    text = "\n".join([page.get_text("text") for page in doc])
    return text

# Endpoint to upload PDF and process text
@app.post("/upload/")
def upload_pdf(file: UploadFile = File(...)):
    if not file.filename.endswith(".pdf"):
        raise HTTPException(status_code=400, detail="Only PDF files are allowed")

    text = extract_text_from_pdf(file)
    if not text.strip():
        raise HTTPException(status_code=400, detail="No text found in PDF")

    # Generate embeddings
    text_embeddings = embedding_model.encode([text])

    # Store document and add embedding to FAISS index
    doc_id = len(documents)
    documents.append(text)
    index.add(np.array(text_embeddings, dtype=np.float32))

    return {"message": "PDF uploaded and processed successfully", "doc_id": doc_id}

# Endpoint to query chatbot
@app.post("/chat/")
def chat_with_bot(query: str):
    if not documents:
        raise HTTPException(status_code=400, detail="No documents uploaded")

    # Convert query to embedding and find nearest document
    query_embedding = embedding_model.encode([query])
    D, I = index.search(np.array(query_embedding, dtype=np.float32), k=1)

    # Retrieve the most relevant document
    relevant_doc = documents[I[0][0]] if I[0][0] >= 0 else "No relevant document found"

    # Use OpenAI GPT to generate a response
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an AI assistant answering questions based on a document."},
            {"role": "user", "content": f"Document: {relevant_doc}\n\nQuestion: {query}"}
        ]
    )
    return {"answer": response['choices'][0]['message']['content']}

# Run using: uvicorn filename:app --reload